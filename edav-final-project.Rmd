--- 
title: "Hurricane Analysis and Visulization Using R"
author: "Romane Goldmuntz, Vy Tran, and Jianqiong Zhan"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [proj.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This is an edav class final project"
---

# Preface

This is a class project written in **Markdown**. We are still working on it.

We are using the **bookdown** package [@R-bookdown] in this project, which was built on top of R Markdown and **knitr** [@xie2015].


<!--chapter:end:index.Rmd-->

# Introduction {#intro}

As coastal shoreline counties create about 40% of the United States’s jobs and account for 46% of its GDP, hurricanes have a trumendous impact on the country’s economy.

They are considered as one of the costliest natural disasters in the world : they currently cost the government over \$28 billion each year, and that amount is expected to increase to over \$39 billion a year due to the increased development of the U.S. coastlines and the global warming. The latter will indeed increase the proportion of cyclones of category 4 and 5, which lead to the most damages and therefore higher costs [@Amadeo2019].  
 
Besides the government, several industries are heavily impacted by hurricanes, including the insurance industry. For example, according to Bloomberg, hurricane Dorian caused the insurance industry losses of up \$25 billion, making it the most expensive natural disaster for the industry since 2017's Hurricane Maria [@DSouza2019].

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). 

![] (https://youtu.be/ezQxonSFCaU)

```{r include=FALSE}
 # keep this chunk in your .Rmd file
 knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}

library(rvest)
library(dplyr)
library(robotstxt)
library(ggplot2)
url <- "https://en.wikipedia.org/wiki/List_of_costliest_Atlantic_hurricanes"
#paths_allowed(url)

df<-as.data.frame(read_html(url) %>% html_table(fill = TRUE))

df_clean <- df %>% mutate(Nominal_Damage = as.factor(gsub("[$><]", "",Nominal.damage.Billions.USD.)))%>%
  select(Name, Season, Storm.classificationat.peak.intensity,Nominal_Damage) %>% rename(Classification = Storm.classificationat.peak.intensity)
df_clean$Season<-as.factor(df_clean$Season)
as.numeric.factor <- function(x) {as.numeric(levels(x))[x]}
df_clean[4]<-lapply(df_clean[4],as.numeric.factor)
df_clean[2]<-lapply(df_clean[2],as.numeric.factor)
df_clean <- df_clean %>% arrange(desc(Season)) %>% top_n(6)
ggplot(df_clean,aes(x=Name, y= Nominal_Damage)) + geom_bar(position = "dodge", stat = "identity") + coord_flip() + xlab("Hurricane") + ylab("Damage (in Billion Dollars)") + ggtitle("Cost of Damage by Six Most Recent Hurricanes") + theme_classic()
```

In addition, hurricane tracking data can provide Federal Emergency Management Agency (FEMA), local emergency managers, and first responders the information they need to be able to send out appropriate responses and help to the citizens at the affected areas [@Newtools4H2019].

For those reasons, hurricanes data is very interesting to analyze and will constitute the topic of this Exploratory Data Analysis and Vizualisation final project.

<!--chapter:end:01-intro.Rmd-->

# Methods

## Data sources
(We describe our data sources, our methods in this chapter)

Storm tracks data can be downloaded from [National Hurricane Center and Central Pacific Hurricane Center](https://www.nhc.noaa.gov/data/#hurdat). The data using in the project is known as Atlantic hurricane database ([HURDAT2](https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atlantic.pdf)) 1851-2018 ([5.9MB download](https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2018-051019.txt)). The data has a comma-delimited, text format with six-hourly information on the location, maximum winds, central pressure, and (beginning in 2004) size of all known tropical cyclones and subtropical cyclones.

## Data transformat

(Describe the process of getting the data into a form in which you could work with it in R.)

```{r include=FALSE}
 # keep this chunk in your .Rmd file
 knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

** load libraries **

```{r}
library(tidyverse)
library(stringr)
# Read in data set
#dfile <- read_lines("https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2018-051019.txt")
dfile<- "data/hurdat2-1851-2018-051019.txt"
hurdat2_in <- read_lines(dfile)
```

The dataset is a combination of serveral subsets. Eeach subset is for a storm track record which incluedes header information and values.

_For instance, the header has the following format:_

AL162018,              OSCAR,     36,
* AL – Basin – Atlantic
* 16 – ATCF cyclone number for that year
* 2018 – Year
* OSCAR – Name, if available, or else “UNNAMED” 
* 36  – Number of best track entries – rows – to follow

_The header is followed by data, which has the following format:_

20181026, 1800,  , SS, 25.4N,  45.3W,  35, 1006,   80,   80,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,

* 2018 – Year
* 10  – Month
* 26  – Day
* 09  – Hours in UTC (Universal Time Coordinate) 35 (Spaces 13-14, before 2nd comma) – Minutes
...

**please refer [this file]("https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atlantic.pdf") for detail information.**

```{r}
header_locations <- (1:length(hurdat2_in))[str_count(hurdat2_in, "\\,") == 3]
header_df <-readr::read_csv(hurdat2_in[header_locations], 
                             col_names = FALSE) %>% 
  dplyr::select(-c("X4"))
names(header_df) <- c("id","name","n_entries")
```

```{r}
# read hearder information
library(stringr)
header_locations <- (1:length(hurdat2_in))[stringr::str_count(hurdat2_in, "\\,") == 3]

header_df <- readr::read_csv(hurdat2_in[header_locations], 
                             col_names = FALSE) %>% 
  dplyr::select(-c("X4"))

names(header_df) <- c("id","name","n_entries")

header_df <-  header_df %>% mutate(header_loc = as.numeric(header_locations))

#tail(header_df)
```



```{r}
# read data value

hurdat2_df <- vector("list", nrow(header_df))
names(hurdat2_df) <- header_df$id
df_names <- c(
  "date", "time", "record_identifier", "status", "latitude", "longitude", "max_wind", "min_pressure",
  "extent_34_NE", "extent_34_SE", "extent_34_SW", "extent_34_NW",
  "extent_50_NE", "extent_50_SE", "extent_50_SW", "extent_50_NW",
  "extent_64_NE", "extent_64_SE", "extent_64_SW", "extent_64_NW", "nas"
)

#
for (i in seq_along(header_df$id)) {
  hurdat2_df[[i]] <- read_csv(dfile,
    skip = header_df$header_loc[i],
    n_max = header_df$n_entries[i],
    col_names = df_names,
    na = c("", "-99", "-999"),
    col_types = list(
      time = col_character(),
      min_pressure = col_integer(),
      extent_34_NE = col_integer(),
      extent_34_SE = col_integer(),
      extent_34_SW = col_integer(),
      extent_34_NW = col_integer(),
      extent_50_NE = col_integer(),
      extent_50_SE = col_integer(),
      extent_50_SW = col_integer(),
      extent_50_NW = col_integer(),
      extent_64_NE = col_integer(),
      extent_64_SE = col_integer(),
      extent_64_SW = col_integer(),
      extent_64_NW = col_integer()
    )
  )
}
```
```{r}
#head(hurdat2_df)
```

```{r}
# Combine and clean the data sets
library(lubridate)
hurdat2 <- 
  hurdat2_df %>%
  dplyr::bind_rows(.id = "id") %>%
  dplyr::mutate(
    date = lubridate::ymd(date),
    year = lubridate::year(date),
    month = lubridate::month(date),
    day = lubridate::day(date),
    hour = as.numeric(stringr::str_sub(time, 1, 2)),
    datetime = as.Date(ISOdate(year, month, day, hour, min = 0, sec = 0, tz = "GMT")),
    #lat_hemisphere = stringr::str_sub(latitude, -1),
    latitude = dplyr::if_else(stringr::str_sub(latitude, -1) == "N",
                              as.numeric(stringr::str_sub(latitude, 1, -2))*1, 
                              as.numeric(stringr::str_sub(latitude, 1, -2))*(-1)),
    longitude = dplyr::if_else(stringr::str_sub(longitude, -1) == "E",
                              as.numeric(stringr::str_sub(longitude, 1, -2))*1, 
                              as.numeric(stringr::str_sub(longitude, 1, -2))*(-1)),
    category = cut(max_wind, # Saffir-Simpson Hurricane Wind Scale
      breaks = c(0, 34, 64, 83, 96, 113, 137, 500),
      labels = c(-1, 0, 1, 2, 3, 4, 5),
      include.lowest = TRUE, ordered = TRUE
    ),
    # wind = wind * 1.15078, # transforms knots to mph,
    TSradius1 = extent_34_NE + extent_34_SW,
    TSradius2 = extent_34_NW + extent_34_SE,
    ts_diameter = pmax(TSradius1, TSradius2) * 1.15078, # to convert from nautical miles to miles # pmax: returns the parallel maxima and minima of the input values
    HUradius1 = extent_64_NE + extent_64_SW,
    HUradius2 = extent_64_NW + extent_64_SE,
    hu_diameter = pmax(HUradius1, HUradius2) * 1.15078, # to convert from nautical miles to miles # pmax: returns the parallel maxima and minima of the input values
    status = recode(status,
                    "TD" = "tropical depression",
                    "TS" = "tropical storm", 
                    "HU" = "tropical hurricane", 
                    "EX" = "Extratropical cyclone", ##
                    "SD" = "subtropical depression",
                    "SS" = "subtropical storm", 
                    "HU" = "tropical hurricane", 
                    "LO" = "a low",
                    "WV" = "tropical wave",
                    "DB" = "disturbance")
  ) 
```

_Note_: category has been calculated based on [Saffir-Simpson Hurricane Wind Scale](https://www.nhc.noaa.gov/aboutsshws.php) to indicate "Types of Damage Due to Hurricane Winds". 

```{r}
# absorb header information to data values
header_df_selected <- header_df %>% select(c("id","name"))
# headers_df_selected
hurdat2_add_name <- left_join(header_df_selected, hurdat2, by=c("id")) %>% 
  select(id, name, datetime, year, month, day, hour, latitude, longitude, status, category,
         max_wind, min_pressure, ts_diameter, hu_diameter)
```

```{r}
hurdat2_out <- hurdat2_add_name %>% 
  mutate(name= dplyr::if_else(grepl("UNNAMED", name), name,
                              stringr::str_to_title(name)))
hurdat2_out$status <- factor(hurdat2_out$status)
hurdat2_out$category <- factor(hurdat2_out$category)
```

```{r}
levels(hurdat2_out$status)
hurdat2_out %>% filter(status == "ET")
```

*Note: there is an "ET" in _Status of system_, which does not included in the description [HURDAT2](https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atlantic.pdf). This is a typo in the dataset, `recode` it into 'EX".*

```{r}
hurdat2_out$status <- dplyr::recode(hurdat2_out$status, ET = "Extratropical cyclone")
```


** Mark storms that have complete pressure record**

```{r}
completeish <- hurdat2_out %>%
  dplyr::group_by(id) %>%
  dplyr::summarise(n_pressure = sum(!is.na(min_pressure)), p_pressure = mean(!is.na(min_pressure))) %>%
  dplyr::filter(p_pressure == 1) %>%
  .[["id"]]
#length(completeish)
#dim(hurdat2_out)[1]
```

Theare are `r length(completeish)` out of `r dim(hurdat2_out)[1]`storms that have complete pressure record.

```{r}
storms_completish_selected <- hurdat2_out %>%
  filter(
    status %in% c("hurricane", "tropical storm", "tropical depression"),
    id %in% completeish)
```

```{r}
hurdat2_out_add_com <- hurdat2_out %>% mutate(completeish = if_else(status %in% c("hurricane", "tropical storm", "tropical depression") & id %in% completeish, "yes","no"))
hurdat2_out_add_com$completeish <- factor(hurdat2_out_add_com$completeish)
```

**Meaning for each variables**

```{r}
names(hurdat2_out_add_com)
```
**_id_**

Storm id, which is unique. A id is a combination of 8 characters, for example, 'AL092011', 
* AL (Spaces 1 and 2) – Basin – Atlantic
* 09 (Spaces 3 and 4) – ATCF cyclone number for that year
* 2011 (Spaces 5-8, before first comma) – Year
for detail information, please see [dataformat](https://www.nhc.noaa.gov/data/hurdat/hurdat2-format-atlantic.pdf)

**_name_**

Storm Name, which is non-unique. There are six lists that are used in rotation and re-cycled every six years, i.e., the 2013 list is used again in 2019. For more information, please see [tropical cyclone names](https://www.nhc.noaa.gov/aboutnames.shtml).

**_datetime, year, month, day, hour_**

Date of report (in Universal Time Coordinate)

**_latitude,longitude_**

Location of storm center

**_status_**

Storm classification (Tropical Depression, Tropical Storm, or Hurricane)

**_category_**

[Saffir-Simpson storm category](https://www.nhc.noaa.gov/aboutsshws.php) (estimated from wind speed. -1 = Tropical Depression, 0 = Tropical Storm)

**_max_wind_**

storm's maximum sustained wind speed (in knots)

**_min_pressure_**

Air pressure at the storm's center (in millibars)

**_ts_diameter_**

Diameter of the area experiencing tropical storm strength winds (34 knots or above)

**_hu_diameter_**

Diameter of the area experiencing hurricane strength winds (64 knots or above)

**_completeish_**

whether storms that have complete pressure record, yes or no


**Save transformed data** for further use.

```{r}
dir <- 'data/'
write_csv(hurdat2_out_add_com, file.path(dir, "hurdat2_out.csv"))
```

## Missing values

Describe any patterns you discover in missing values.

```{r echo=FALSE}
library(tidyverse)
library(stringr)
library(naniar)
library(mi)
#devtools::install_github("cran/extracat", force = TRUE)
#devtools::install_github("coatless/ucidata", force = TRUE)
library(extracat)
library(visdat)
```

```{r}
# Read in data set
#dfile <- read_lines("https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2018-051019.txt")
dfile<- "data/hurdat2_out.csv"
#dfile <- "https://github.com/jqz300/edav_proj/blob/master/data/hurdat2_out.csv"
df <- read_csv(dfile,
                   na = c("NA", "-99", "-999"),
                   col_types = list(
                     id = col_character(),
                   name = col_character(),
                   year =col_integer(),
                   month =col_integer(),
                   day = col_integer(),
                   hour  =col_integer(),
                   latitude  = col_double(),
                   longitude   = col_double(),
                   status  = col_factor(),
                   category = col_factor(),
                   max_wind  = col_double(),
                   min_pressure  = col_integer(),
                   ts_diameter = col_double(),
                   hu_diameter = col_double(),
                   completeish = col_factor()))
```

```{r}
library(dplyr)
missing <- colSums(is.na(df)) %>%
        sort(decreasing = TRUE) %>% as.data.frame()
names(missing) <- "n_missing"
missing <- missing %>% rownames_to_column(var="variables") %>% dplyr::mutate(p_missing = n_missing/nrow(df))
head(missing)
```

```{r}
missing %>% #filter(missing>0) %>% 
        ggplot(aes(x=reorder(variables,p_missing), y=p_missing))+
        geom_col()+
        coord_flip()+
        labs(title = "Percentage of Missing Value By Variable") +
        xlab('Variable')+
        ylab('Percentage of Missing Value')+
        theme(plot.title = element_text(size=12), axis.text.x=element_text(angle = 0,vjust = 0.5,size = 8))
```

_comments_

* only _hu_diameter_, _ts_diameter_, _min_pressure_, _max_wind_, _category_, and _longitude_ have missing values. Among them, `r missing$variables[1]` and `r missing$variables[2]` have the highest number of missing values with total number of missing values of `r missing$n_missing[1]`, meaning  `r missing$p_missing[1]*100` of data are missing. The rest observations are with no missing values.


```{r}
# review missingness patterns
extracat::visna(df, sort = "b")
```

_Comments_

_Observe by column:_

* _The bars_ beneath the columns show the proportions of missingness by variable, suggesting _hu_diameter_ and _ts_diameter_ both have the highest number of missing value and the columns suggest that they follow the same missing pattern, meaning when _hu_diameter_ is missing, _ts_diameter_ is also missing. The third most missing variable is _min_pressure_ and _the columns_ show that _min_pressure_ is missing only when _hu_diameter_ and _ts_diameter_ are missing. This missing pattern can be applied to _category_ and _max_wind_ as well, meaning the missing value for _category_ and _max_wind_ can only be found when _hu_diameter_, _ts_diameter_ and _min_pressure_ are missing. However, the missing value for _longitude_ can be found when _hu_diameter_ and _ts_diameter_ are not missing.

_Observe by row:_

* _The bars_ on the right show the relative frequencies of the patterns suggesting the most frequent missing patterns are in the combination of _hu_diameter_, _ts_diameter_ and _min_pressure_, followed by the combination of _hu_diameter_ and _ts_diameter_. non-missing data are in the third meaning most of rows are completeness.


```{r}
visdat::vis_miss(df)
```

_Comments_

* Most of missing values are in the earlier observations.


```{r}
visdat::vis_dat(df)
```

_Comments_

* The most prevalent missing data types are eithor_numeric_ or _integer_ and only a few missing values are _factors_


```{r}
df_selected <- df %>% dplyr::select(-c("month","day","hour")) %>% 
        dplyr::filter(year>1990)
```

```{r,echo=T,eval=TRUE,warning=FALSE,cache=T}
df_completeish <- 
  df %>% group_by(year) %>% summarize(num_completeish = n(), num_na = sum(is.na(min_pressure))) %>% mutate(percent_na = num_na/num_completeish) %>% arrange(-percent_na)

g1<-ggplot(df_completeish, aes(x=year, y =percent_na))+
  geom_line()+
  labs(title = "Graph1. Proportion of Missing 'Min_pressure' By year")+
  xlab('year')+ylab("Proportion of Missing 'Min_pressure'")

```

```{r,echo=T,eval=TRUE,warning=FALSE,cache=T}
df_status <- df %>% group_by(status) %>% summarize(num_status = n(), num_na = sum(is.na(min_pressure))) %>% mutate(percent_na = num_na/num_status) %>% arrange(-percent_na)

g2<-ggplot(df_status, aes(x=reorder(status,-percent_na), y =percent_na))+
  geom_col()+
  labs(title = "Graph2. Proportion of Missing 'Min_pressure' By Status")+
  xlab('Status')+ylab("Proportion of Missing 'Min_pressure'")+
  coord_flip()
```


```{r,echo=T,eval=TRUE,warning=FALSE,cache=T}
df_category <- df %>% group_by(category) %>% summarize(num_category = n(), num_na = sum(is.na(min_pressure))) %>% mutate(percent_na = num_na/num_category) %>% arrange(-percent_na)

g3<-ggplot(df_category, aes(x=reorder(category,-percent_na), y =percent_na))+
  geom_col()+
  labs(title = "Graph1. Proportion of Missing 'Min_pressure' By Category")+
  xlab('Category')+ylab("Proportion of Missing 'Min_Pressure'")
```

```{r}
g1
g2
g3
```

_Comments_

* Graph1, _missing value vs. year_, shows that almost all _Min_pressure_ data are missing from 1850s to 1940s, number of missing data is decreasing since the 1940s and there are no missing _Min_pressure_ since the 2000s.

* Graph2, _missing value vs. status_, shows that _tropical depression_ has the highest missing number of _min_pressure_ and _a low_ has the lowest missing number of _min_pressure_.

* Graph3, _missing value vs. category_, shows that if _min_pressure_ is missing, category is missing. Category two has the highest missing number of _min_pressure_, while category five has the lowest missing number of _min_pressure_. Interesting, as the storms become more dangerous, more data are available. Here, category two means _Extremely dangerous winds will cause extensive damage_, while category five means _Catastrophic damage will occur_.

```{r}
df %>% filter(year>=1940) %>% dplyr::mutate(decade=cut(year, 
                                                      breaks=c(1940, 1950, 1960, 1970, 1980, 1990,
                                                               2000, 2010, 2020),
                                                      #labels=c('40s','50s','60s','70s','80s','90s',
                                                      #         '00s','10s','20s'),
                                                      include.lowest = TRUE, ordered=TRUE)) %>% 
naniar::gg_miss_var(facet=decade, show_pct = TRUE) + labs(y="Mising by Time")

```

_comments_

* _Hu_diameter_ and _ts_diamter_ are the most prevalent missing variable for each decade. 
* Percentage of missing values in _ts_diameter_ and _hu_diameter_ have stayed the same before 2000, and decreased to 27% in 2000s. There are no missing values in 2010s.
* Percentage of missing values in _min_pressure_ have decreased since the 1940s and with no missing values since 1990s.
* Percentage of missing values in _max_wind_ only appear in 1960s and 1980s.
* There are missing values in the category in the 1960s.

```{r}
naniar::gg_miss_var(df, facet=status, show_pct = TRUE) + labs(y="Missing by Status")
```

_Comments_

*  _Hu_diameter_ and _ts_diamter_ are the most prevalent missing variable for each status. 
* _Missing value facet by status_ shows that _topical depression_ have the highest number of missing value, while _a low_ barely have any missing values. Most missing values are in _ts_diameter_, _hu_diameter, and _min_pressure_.
* There are _tropical depression_ with missing values in _max_wind_ and _category_.


```{r}
naniar::gg_miss_var(df, facet=category, show_pct = TRUE) + labs(y="Missing by Category")
```

_Comments_

* _Missing value facet by category_ shows that _hu_diameter_, _ts_diameter_ and _min_pressure_ are the most prevalent missing variable for each category.

```{r}
dfsum_category <- df %>% group_by(category) %>% summarize(year = round(mean(min_pressure, na.rm=TRUE),1),
                                                 #month = round(mean(min_pressure, na.rm=TRUE),1),
                                                 min_pressure = round(mean(min_pressure, na.rm=TRUE),1),
                                                 max_wind = round(mean(max_wind, na.rm=TRUE),1),
                                                 hu_diameter=round(mean(hu_diameter, na.rm=TRUE),1),
                                                 ts_diameter=round(mean(hu_diameter, na.rm=TRUE),1)) %>% 
        left_join(df_category %>% select(category, percent_na), by="category") %>% 
        arrange(desc(percent_na)) %>% filter(category != "NA")
```
```{r}
library(GGally)
ggpairs(dfsum_category, columns=c(2,3,4,5,7), aes(color=category))
```

_Comments_

* The number of missing values for the _catagory_ column seems to be correlated with _min_pressure_: there are more missing values in _catagory_ for high _min_pressure_
* There are more missing values in the _catagory_ column for low _max_wind_.
* There are more missing values in the _catagory_ column for low _hu_diameter_.
* There are more missing values in the _catagory_ column in earlier _years_.


```{r}
dfsum_status <- df %>% group_by(status) %>% summarize(year = round(mean(min_pressure, na.rm=TRUE),1),
                                                 #month = round(mean(min_pressure, na.rm=TRUE),1),
                                                 min_pressure = round(mean(min_pressure, na.rm=TRUE),1),
                                                 max_wind = round(mean(max_wind, na.rm=TRUE),1),
                                                 hu_diameter=round(mean(hu_diameter, na.rm=TRUE),1),
                                                 ts_diameter=round(mean(hu_diameter, na.rm=TRUE),1)) %>% 
        left_join(df_status %>% select(status, percent_na), by="status") %>% 
        arrange(desc(percent_na)) %>% filter(status != "NA")
```
```{r}
library(GGally)
ggpairs(dfsum_status, columns=c(3,5, 7), aes(color=status))
```

_Comments_

* The number of missing values for the _status_ column seems to be correlated with _min_pressure_: there are more missing values in _status_ for low _min_pressure_
* There are more missing values in the _status_ column for high _hu_diamter_.


```{r}
df <- df %>% dplyr::mutate(decade=cut(year,
                                breaks=c(1850, 1860, 1870, 1880, 1890, 1900, 1910, 1920, 1930, 1940,
                                         1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020),
                                include.lowest = TRUE, ordered=TRUE))

df_decade <- df %>% group_by(decade) %>% summarize(num_decade = n(), num_na = sum(is.na(min_pressure))) %>% mutate(percent_na = num_na/num_decade) %>% arrange(-percent_na)
```

```{r}
dfsum_decade <- df %>% group_by(decade) %>% summarize(#year = round(mean(min_pressure, na.rm=TRUE),1),
                                                 #month = round(mean(min_pressure, na.rm=TRUE),1),
                                                 min_pressure = round(mean(min_pressure, na.rm=TRUE),1),
                                                 max_wind = round(mean(max_wind, na.rm=TRUE),1),
                                                 hu_diameter=round(mean(hu_diameter, na.rm=TRUE),1),
                                                 ts_diameter=round(mean(hu_diameter, na.rm=TRUE),1)) %>% 
        left_join(df_decade %>% select(decade, percent_na), by="decade") %>% 
        arrange(desc(percent_na)) %>% filter(decade != "NA")
```


```{r}
library(GGally)
ggpairs(dfsum_decade, columns=c(2,3,6), aes(color=decade))
```
_comments_

* The number of missing values for the _decade_ column seems to be correlated with _min_pressure_: there are more missing values in _decade_ for low _min_pressure_

* There are more missing values in the _decade_ column for high _max_wind_.

<!--chapter:end:02-method.Rmd-->

# Results

Provide a short nontechnical but _significant_ summary of the most revealing findings of our analysis written for a nontechnical audience. Take extra care to clean up our graphs, ensuring that best practices for presentation are followed, as described in the audience ready style section below.


```{r include=FALSE}
 # keep this chunk in your .Rmd file
 knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Read tranformed data

```{r echo=FALSE}
library(tidyverse)
library(stringr)
```

```{r}
# Read in transformed data
#dfile<- "data/hurdat2_out.csv"
dfile<- "https://github.com/jqz300/edav_proj/blob/master/data/hurdat2_out.csv"
df <- read_csv(dfile,
                   na = c("NA", "-99", "-999"),
                   col_types = list(
                     id = col_character(),
                   name = col_character(),
                   year =col_integer(),
                   month =col_integer(),
                   day = col_integer(),
                   hour  =col_integer(),
                   latitude  = col_double(),
                   longitude   = col_double(),
                   status  = col_factor(),
                   category = col_factor(),
                   max_wind  = col_double(),
                   min_pressure  = col_integer(),
                   ts_diameter = col_double(),
                   hu_diameter = col_double(),
                   completeish = col_factor()))
```

```{r}
tail(df)
```

## Figures

```{r hex-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
df %>% 
  #filter(completeish == "yes") %>% 
  ggplot(aes(x=longitude, y=latitude))+
  geom_hex()+
  facet_wrap(~category, ncol=3)+
  labs(title = "Fig title")

# need to add map
```

Figure \@ref(fig:hex-fig) shows


```{r boxplot-wind-category-lat-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}

library(gridExtra)

df %>%
        ggplot()+
        geom_boxplot(aes(x=reorder(category, max_wind, median), y=latitude), varwidth = TRUE)+
        #coord_flip()
        facet_wrap(~status, scale="free")
```

```{r boxplot-wind-category-hu-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}

library(gridExtra)

df %>%
        ggplot()+
        geom_boxplot(aes(x=reorder(category, max_wind, median), y=hu_diameter), varwidth = TRUE)+
        #coord_flip()
        #facet_wrap(~status)#, scale="free")+
        labs(title = "Fig title")
```


```{r ridges-year-category-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggridges)
library(viridis)
df %>%
        ggplot()+
        geom_density_ridges_gradient(aes(x= year, y= category, group = category, fill = ..x.., scale = 0.9))+
        scale_fill_viridis()+
        #coord_flip()+
        labs(title = "Fig title")
```

```{r density-year-status-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggridges)
library(viridis)
df %>%
        ggplot()+
        geom_density_ridges_gradient(aes(x= year, y= status, group = status, fill = ..x.., scale = 0.9))+
        scale_fill_viridis()+
        #coord_flip()+
        #facet_wrap(~cluster_name_more_short, scale="free", ncol = 1)+
        labs(title = "Fig title")
```

```{r cleveland-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
# cleveland

```

```{r parcoords-fig, fig.cap='Here is a nice figure!', fig.align='center'}
library(parcoords)

#df %>%
#        dplyr::select( ts_diameter, hu_diameter, min_pressure, max_wind, category, status) %>% 
#  drop_na() %>% 
#        parcoords(alpha = 0.2,
#                  color = list(colorBy = "category"),
#                  withD3 = TRUE,
#                  rownames = FALSE,
#                  reorderable = TRUE,
#                  brushMode = "1d-axes")
```


```{r point-date-wind-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
df %>%
  drop_na() %>% filter(name =="Katrina") %>% 
  ggplot(aes(x=longitude, y=latitude, color=ts_diameter))+
  geom_point()

## need to add map

df %>%
  drop_na() %>% filter(name =="Katrina") %>% 
  ggplot(aes(x=datetime, y=max_wind, color=category))+
  geom_point()
  
  
```

```{r plotly-date-wind-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}

library(plotly)
#df %>%
#  drop_na() %>% 
#  filter(name =="Katrina") %>%
#  plot_ly(x=~datetime, y=~max_wind)
```
```{r}
levels(df %>% filter(completeish=="yes") %>% .$status)
```

```{r mosaic-category-status-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
#names(df)
df_selected <- df %>% 
  #filter(completeish == "yes") %>% 
  filter(year>1990) %>% 
  #filter (status == c("hurricane","tropical storm","tropical depression")) %>%
  filter(hu_diameter>0) %>% 
  drop_na() %>% 
  dplyr::mutate(decade=cut(year,
                           breaks = c(2000, 2005, 2010, 2015, 2020),
                           lables = c('00s', '05s','10s','15', '20s'),
                           include.lowest = TRUE, ordered=TRUE))
```


```{r mosaic-hdecade-category-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggmosaic)
ggplot(data = df_selected) +
   geom_mosaic(aes(x = product(category, decade), fill=category), na.rm=TRUE)
```

```{r mosaic-category-month-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
library(ggmosaic)
ggplot(data = df_selected) +
   geom_mosaic(aes(x = product(category, month), fill=category), na.rm=TRUE)
```

```{r year-count-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
df %>%
  select(c("year", "id", "status")) %>% 
  unique() %>% 
  group_by(year, status) %>% 
  drop_na() %>% 
  count() %>% 
  ungroup() %>% 
  ggplot(aes(x=year, y=n))+
  geom_line()+
  facet_wrap(~status, scale="free")
```

## Tables

Summary table if applicable

We can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). 
And it works even the item is not in this chapter.


```{r nice-tab, tidy=FALSE}

knitr::kable(
  head(df_selected[2:3], 10), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

<!--chapter:end:03-results.Rmd-->

# Interactive Component

Select one (or more) of our key findings to present in an interactive format (D3). Be selective in the choices that we present to the user; the idea is that in 5-10 minutes, users should have a good sense of the question(s) that we are interested in and the trends we’ve identified in the data. In other words, they should understand the value of the analysis, be it business value, scientific value, general knowledge, etc.

<!--chapter:end:04-interactive.Rmd-->

# Conclusion

Discuss limitations and future directions, lessons learned.

<!--chapter:end:05-conclusion.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

