[
["index.html", "Hurricane Analysis and Visulization Using R Chapter 1 Preface", " Hurricane Analysis and Visulization Using R Romane Goldmuntz, Vy Tran, and Jianqiong Zhan 2019-11-07 Chapter 1 Preface This is a class project written in Markdown. We are still working on it. We are using the bookdown package (Xie 2019) in this project, which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction As coastal shoreline counties create about 40% of the United States’s jobs and account for 46% of its GDP, hurricanes have a trumendous impact on the country’s economy. They are considered as one of the costliest natural disasters in the world : they currently cost the government over $28 billion each year, and that amount is expected to increase to over $39 billion a year due to the increased development of the U.S. coastlines and the global warming. The latter will indeed increase the proportion of cyclones of category 4 and 5, which lead to the most damages and therefore higher costs (Amadeo 2019). Besides the government, several industries are heavily impacted by hurricanes, including the insurance industry. For example, according to Bloomberg, hurricane Dorian caused the insurance industry losses of up $25 billion, making it the most expensive natural disaster for the industry since 2017’s Hurricane Maria (D’Souza 2019). library(rvest) library(dplyr) library(robotstxt) library(ggplot2) url &lt;- &quot;https://en.wikipedia.org/wiki/List_of_costliest_Atlantic_hurricanes&quot; #paths_allowed(url) df&lt;-as.data.frame(read_html(url) %&gt;% html_table(fill = TRUE)) df_clean &lt;- df %&gt;% mutate(Nominal_Damage = as.factor(gsub(&quot;[$&gt;&lt;]&quot;, &quot;&quot;,Nominal.damage.Billions.USD.)))%&gt;% select(Name, Season, Storm.classificationat.peak.intensity,Nominal_Damage) %&gt;% rename(Classification = Storm.classificationat.peak.intensity) df_clean$Season&lt;-as.factor(df_clean$Season) as.numeric.factor &lt;- function(x) {as.numeric(levels(x))[x]} df_clean[4]&lt;-lapply(df_clean[4],as.numeric.factor) df_clean[2]&lt;-lapply(df_clean[2],as.numeric.factor) df_clean &lt;- df_clean %&gt;% arrange(desc(Season)) %&gt;% top_n(6) ggplot(df_clean,aes(x=Name, y= Nominal_Damage)) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) + coord_flip() + xlab(&quot;Hurricane&quot;) + ylab(&quot;Damage (in Billion Dollars)&quot;) + ggtitle(&quot;Cost of Damage by Six Most Recent Hurricanes&quot;) + theme_classic() In addition, hurricane tracking data can provide Federal Emergency Management Agency (FEMA), local emergency managers, and first responders the information they need to be able to send out appropriate responses and help to the citizens at the affected areas (GOES-r 2019). For those reasons, hurricanes data is very interesting to analyze and will constitute the topic of this Exploratory Data Analysis and Vizualisation final project. References "],
["methods.html", "Chapter 3 Methods 3.1 Data sources 3.2 Data transformat 3.3 Missing values", " Chapter 3 Methods 3.1 Data sources (We describe our data sources, our methods in this chapter) Storm tracks data can be downloaded from National Hurricane Center and Central Pacific Hurricane Center. The data using in the project is known as Atlantic hurricane database (HURDAT2) 1851-2018 (5.9MB download). The data has a comma-delimited, text format with six-hourly information on the location, maximum winds, central pressure, and (beginning in 2004) size of all known tropical cyclones and subtropical cyclones. 3.2 Data transformat Describe the process of getting the data into a form in which you could work with it in R. library(tidyverse) library(stringr) # Read in data set #dfile &lt;- read_lines(&quot;https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2018-051019.txt&quot;) dfile&lt;- &quot;data/hurdat2-1851-2018-051019.txt&quot; storm_strings &lt;- read_lines(dfile) # Identify the header lines that have three commas library(stringr) header_locations &lt;- (1:length(storm_strings))[str_count(storm_strings, &quot;\\\\,&quot;) == 3] # count # of strings for each line, the header line has only 3 strings. headers &lt;- as.list(storm_strings[header_locations]) headers_df &lt;- headers %&gt;% purrr::map(stringr::str_sub, start = 1L, end = -2L) %&gt;% # to remove trailing comma purrr::map(paste0, &quot;\\n&quot;) %&gt;% # and replace final comma with \\n purrr::map_dfr(read_csv, col_names = c(&quot;id&quot;, &quot;name&quot;, &quot;n_obs&quot;)) %&gt;% dplyr::mutate(#name = dplyr::recode(name, &quot;UNNAMED&quot; = id), skip = header_locations) %&gt;% dplyr::select(id, name, skip, n_obs) # Set data frames names df_names &lt;- c( &quot;date&quot;, &quot;time&quot;, &quot;record_type&quot;, &quot;status&quot;, &quot;lat&quot;, &quot;long&quot;, &quot;wind&quot;, &quot;pressure&quot;, &quot;extent_34_NE&quot;, &quot;extent_34_SE&quot;, &quot;extent_34_SW&quot;, &quot;extent_34_NW&quot;, &quot;extent_50_NE&quot;, &quot;extent_50_SE&quot;, &quot;extent_50_SW&quot;, &quot;extent_50_NW&quot;, &quot;extent_64_NE&quot;, &quot;extent_64_SE&quot;, &quot;extent_64_SW&quot;, &quot;extent_64_NW&quot;, &quot;nas&quot; ) storm_dfs &lt;- vector(&quot;list&quot;, nrow(headers_df)) names(storm_dfs) &lt;- headers_df$id # Read in the sub-datasets as data frames for (i in seq_along(headers_df$name)) { storm_dfs[[i]] &lt;- read_csv(dfile, skip = headers_df$skip[i], n_max = headers_df$n_obs[i], col_names = df_names, na = c(&quot;&quot;, &quot;-99&quot;, &quot;-999&quot;), col_types = list( time = col_character(), pressure = col_integer(), extent_34_NE = col_integer(), extent_34_SE = col_integer(), extent_34_SW = col_integer(), extent_34_NW = col_integer(), extent_50_NE = col_integer(), extent_50_SE = col_integer(), extent_50_SW = col_integer(), extent_50_NW = col_integer(), extent_64_NE = col_integer(), extent_64_SE = col_integer(), extent_64_SW = col_integer(), extent_64_NW = col_integer() ) ) } # Combine and clean the data sets library(lubridate) storms &lt;- storm_dfs %&gt;% dplyr::bind_rows(.id = &quot;id&quot;) %&gt;% dplyr::mutate( date = lubridate::ymd(date), year = lubridate::year(date), month = lubridate::month(date), day = lubridate::day(date), hour = as.numeric(stringr::str_sub(time, 1, 2)), datetime = ISOdate(year, month, day, hour, min = 0, sec = 0, tz = &quot;GMT&quot;), lat_hemisphere = stringr::str_sub(lat, -1), lat_sign = dplyr::if_else(lat_hemisphere == &quot;N&quot;, 1, -1), lat = as.numeric(stringr::str_sub(lat, 1, -2)) * lat_sign, long_hemisphere = stringr::str_sub(long, -1), long_sign = dplyr::if_else(long_hemisphere == &quot;E&quot;, 1, -1), long = as.numeric(stringr::str_sub(long, 1, -2)) * long_sign, category = cut(wind, #cut divides the range of x into intervals breaks = c(0, 34, 64, 83, 96, 113, 137, 500), labels = c(-1, 0, 1, 2, 3, 4, 5), include.lowest = TRUE, ordered = TRUE ), # wind = wind * 1.15078, # transforms knots to mph, TSradius1 = extent_34_NE + extent_34_SW, TSradius2 = extent_34_NW + extent_34_SE, ts_diameter = pmax(TSradius1, TSradius2) * 1.15078, # to convert from nautical miles to miles # pmax: returns the parallel maxima and minima of the input values HUradius1 = extent_64_NE + extent_64_SW, HUradius2 = extent_64_NW + extent_64_SE, hu_diameter = pmax(HUradius1, HUradius2) * 1.15078, # to convert from nautical miles to miles status = recode(status, &quot;TD&quot; = &quot;tropical depression&quot;, &quot;TS&quot; = &quot;tropical storm&quot;, &quot;HU&quot; = &quot;tropical hurricane&quot;, &quot;EX&quot; = &quot;Extratropical cyclone&quot;, ## &quot;SD&quot; = &quot;subtropical depression&quot;, &quot;SS&quot; = &quot;subtropical storm&quot;, &quot;HU&quot; = &quot;tropical hurricane&quot;, &quot;LO&quot; = &quot;a low&quot;, &quot;WV&quot; = &quot;tropical wave&quot;, &quot;DB&quot; = &quot;disturbance&quot;) ) # data output headers_df_selected &lt;- headers_df %&gt;% select(c(&quot;id&quot;,&quot;name&quot;)) # headers_df_selected storms_with_name &lt;- left_join(headers_df_selected, storms, by=c(&quot;id&quot;)) %&gt;% select(id, name, datetime, year, month, day, hour, lat, long, status, category, wind, pressure, extent_34_NE, extent_34_SW, extent_34_NW, extent_34_SE, extent_50_NE, extent_50_SW, extent_50_NW, extent_50_SE, extent_64_NE, extent_64_SW, extent_64_NW, extent_64_SE, ts_diameter, hu_diameter) storms_all &lt;- storms_with_name %&gt;% mutate(name= dplyr::if_else(grepl(&quot;UNNAMED&quot;, name), name, stringr::str_to_title(name))) storms_all$status &lt;- factor(storms_all$status) storms_all$id &lt;- factor(storms_all$id) storms_all$name &lt;- factor(storms_all$name) levels(storms_all$status) ## [1] &quot;a low&quot; &quot;disturbance&quot; &quot;ET&quot; ## [4] &quot;Extratropical cyclone&quot; &quot;subtropical depression&quot; &quot;subtropical storm&quot; ## [7] &quot;tropical depression&quot; &quot;tropical hurricane&quot; &quot;tropical storm&quot; ## [10] &quot;tropical wave&quot; storms_all %&gt;% filter(status == &quot;ET&quot;) ## # A tibble: 1 x 27 ## id name datetime year month day hour lat long status ## &lt;fct&gt; &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 AL09… Harv… 1993-09-21 18:00:00 1993 9 21 18 46 -42 ET ## # … with 17 more variables: category &lt;ord&gt;, wind &lt;dbl&gt;, pressure &lt;int&gt;, ## # extent_34_NE &lt;int&gt;, extent_34_SW &lt;int&gt;, extent_34_NW &lt;int&gt;, ## # extent_34_SE &lt;int&gt;, extent_50_NE &lt;int&gt;, extent_50_SW &lt;int&gt;, ## # extent_50_NW &lt;int&gt;, extent_50_SE &lt;int&gt;, extent_64_NE &lt;int&gt;, ## # extent_64_SW &lt;int&gt;, extent_64_NW &lt;int&gt;, extent_64_SE &lt;int&gt;, ## # ts_diameter &lt;dbl&gt;, hu_diameter &lt;dbl&gt; Note: there is an “ET” in Status of system, however, there is no description in HURDAT2, which is a typo in the dataset, recode it into ’EX&quot;. storms_all$status &lt;- dplyr::recode(storms_all$status, ET = &quot;Extratropical cyclone&quot;) #levels(factor(storms_all$status)) Narrow to storms that have complete pressure record # Narrow to storms that have complete pressure record completeish &lt;- storms_all %&gt;% dplyr::group_by(id) %&gt;% dplyr::summarise(n_pressure = sum(!is.na(pressure)), p_pressure = mean(!is.na(pressure))) %&gt;% dplyr::filter(p_pressure == 1) %&gt;% .[[&quot;id&quot;]] #length(completeish) dim(storms_all)[1] ## [1] 50911 Theare are 562 out of 50911storms that have complete pressure record. storms_completish_selected &lt;- storms_all %&gt;% filter( status %in% c(&quot;hurricane&quot;, &quot;tropical storm&quot;, &quot;tropical depression&quot;), id %in% completeish) storms_all_add_com &lt;- storms_all %&gt;% mutate(completeish = if_else(status %in% c(&quot;hurricane&quot;, &quot;tropical storm&quot;, &quot;tropical depression&quot;) &amp; id %in% completeish, &quot;yes&quot;,&quot;no&quot;)) storms_all_add_com$completeish &lt;- factor(storms_all_add_com$completeish) Save transformed data dir &lt;- &#39;data/&#39; write_csv(storms_all_add_com, file.path(dir, &quot;storms_all_out.csv&quot;)) dir &lt;- &#39;data/&#39; write_csv(storms, file.path(dir, &quot;storms_completish_selected.csv&quot;)) 3.3 Missing values Describe any patterns you discover in missing values. "],
["results.html", "Chapter 4 Results", " Chapter 4 Results Provide a short nontechnical but significant summary of the most revealing findings of our analysis written for a nontechnical audience. Take extra care to clean up our graphs, ensuring that best practices for presentation are followed, as described in the audience ready style section below. "],
["interactive-component.html", "Chapter 5 Interactive Component", " Chapter 5 Interactive Component Select one (or more) of our key findings to present in an interactive format (D3). Be selective in the choices that we present to the user; the idea is that in 5-10 minutes, users should have a good sense of the question(s) that we are interested in and the trends we’ve identified in the data. In other words, they should understand the value of the analysis, be it business value, scientific value, general knowledge, etc. "],
["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion Discuss limitations and future directions, lessons learned. "],
["references.html", "References", " References "]
]
